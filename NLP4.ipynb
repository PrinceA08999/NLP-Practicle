{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOyjKQSS70hL5kYsD/seyto"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"p1xzmpPLFQD4","executionInfo":{"status":"ok","timestamp":1714046046016,"user_tz":-330,"elapsed":4298,"user":{"displayName":"Prince A (Adhikesh Somwanshi)","userId":"09578747815701702608"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","class Transformer(nn.Module):\n","    def __init__(self, input_dim, output_dim, hid_dim, n_layers, n_heads, pf_dim, dropout):\n","        super().__init__()\n","\n","        self.encoder_layers = nn.ModuleList([EncoderLayer(hid_dim, n_heads, pf_dim, dropout) for _ in range(n_layers)])\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.input_dim = input_dim\n","        self.output_dim = output_dim\n","\n","    def forward(self, src):\n","        # src = [batch size, src len, input dim]\n","\n","        for layer in self.encoder_layers:\n","            src = layer(src)\n","\n","        # src = [batch size, src len, hid dim]\n","\n","        output = self.fc_out(src[:, -1, :])\n","\n","        return output\n","\n","\n","class EncoderLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, pf_dim, dropout):\n","        super().__init__()\n","\n","        self.self_attn_layer_norm = nn.LayerNorm(hid_dim)\n","        self.fc_layer_norm = nn.LayerNorm(hid_dim)\n","        self.self_attention = MultiHeadAttentionLayer(hid_dim, n_heads, dropout)\n","        self.positionwise_feedforward = PositionwiseFeedforwardLayer(hid_dim, pf_dim, dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        # src = [batch size, src len, hid dim]\n","\n","        # self attention\n","        _src, _ = self.self_attention(src, src, src)  # all source\n","\n","        # dropout, residual connection and layer norm\n","        src = self.self_attn_layer_norm(src + self.dropout(_src))\n","\n","        # src = [batch size, src len, hid dim]\n","\n","        # positionwise feedforward\n","        _src = self.positionwise_feedforward(src)\n","\n","        # dropout, residual and layer norm\n","        src = self.fc_layer_norm(src + self.dropout(_src))\n","\n","        # src = [batch size, src len, hid dim]\n","\n","        return src\n","class MultiHeadAttentionLayer(nn.Module):\n","    def __init__(self, hid_dim, n_heads, dropout):\n","        super().__init__()\n","        assert hid_dim % n_heads == 0\n","        self.hid_dim = hid_dim\n","        self.n_heads = n_heads\n","        self.head_dim = hid_dim // n_heads\n","        self.fc_q = nn.Linear(hid_dim, hid_dim)\n","        self.fc_k = nn.Linear(hid_dim, hid_dim)\n","        self.fc_v = nn.Linear(hid_dim, hid_dim)\n","        self.fc_o = nn.Linear(hid_dim, hid_dim)\n","        self.dropout = nn.Dropout(dropout)\n","        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to(device)\n","    def forward(self, query, key, value, mask=None):\n","        batch_size = query.shape[0]\n","        # query = [batch size, query len, hid dim]\n","        # key = [batch size, key len, hid dim]\n","        # value = [batch size, value len, hid dim]\n","        Q = self.fc_q(query)\n","        K = self.fc_k(key)\n","        V = self.fc_v(value)\n","        # Q = [batch size, query len, hid dim]\n","        # K = [batch size, key len, hid dim]\n","        # V = [batch size, value len, hid dim]\n","        Q = Q.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        K = K.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","        V = V.view(batch_size, -1, self.n_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","        # Q = [batch size, n heads, query len, head dim]\n","        # K = [batch size, n heads, key len, head dim]\n","        # V = [batch size, n heads, value len, head dim]\n","        energy = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n","        # energy = [batch size, n heads, query len, key len]\n","        if mask is not None:\n","             energy = energy.masked_fill(mask == 0, -1e10)\n","             attention = torch.softmax(energy, dim=-1)\n","        # attention = [batch size, n heads, query len, key len]\n","        x = torch.matmul(self.dropout(attention), V)\n","        # x = [batch size, n heads, query len, head dim]\n","        x = x.permute(0, 2, 1, 3).contiguous()\n","        # x = [batch size, query len, n heads, head dim]\n","        x = x.view(batch_size, -1, self.hid_dim)\n","        # x = [batch size, query len, hid dim]\n","        x = self.fc_o(x)\n","        # x = [batch size, query len, hid dim]\n","        return x, attention\n","\n","\n","class PositionwiseFeedforwardLayer(nn.Module):\n","    def __init__(self, hid_dim, pf_dim, dropout):\n","        super().__init__()\n","\n","        self.fc_1 = nn.Linear(hid_dim, pf_dim)\n","        self.fc_2 = nn.Linear(pf_dim, hid_dim)\n","\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        # x = [batch size, seq len, hid dim]\n","\n","        x = self.dropout(torch.relu(self.fc_1(x)))\n","\n","        # x = [batch size, seq len, pf dim]\n","\n","        x = self.fc_2(x)\n","\n","        # x = [batch size, seq len, hid dim]\n","\n","        return x\n"]}]}