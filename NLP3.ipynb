{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNh29PER4yl2nijsPFnEk4l"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_r1vPWCxD5sJ","executionInfo":{"status":"ok","timestamp":1714045573919,"user_tz":-330,"elapsed":2407,"user":{"displayName":"Prince A (Adhikesh Somwanshi)","userId":"09578747815701702608"}},"outputId":"22e6969a-dfd9-42dd-a571-b00d994f132e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Outputs saved successfully.\n"]}],"source":["import nltk\n","import pandas as pd\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import WordNetLemmatizer\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","\n","# Sample data\n","data = {\n","    'text': [\"This is the first document.\",\n","             \"This document is the second document.\",\n","             \"And this is the third one.\",\n","             \"Is this the first document?\"],\n","    'label': ['A', 'B', 'C', 'A']\n","}\n","\n","# Convert data to DataFrame\n","df = pd.DataFrame(data)\n","\n","# Text Cleaning, Lemmatization, and Stop Words Removal\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words('english'))\n","\n","def clean_text(text):\n","    tokens = word_tokenize(text.lower())  # Tokenize and convert to lowercase\n","    tokens = [token for token in tokens if token.isalnum()]  # Remove non-alphanumeric characters\n","    tokens = [lemmatizer.lemmatize(token) for token in tokens]  # Lemmatize\n","    tokens = [token for token in tokens if token not in stop_words]  # Remove stop words\n","    return ' '.join(tokens)\n","\n","df['clean_text'] = df['text'].apply(clean_text)\n","\n","# Label Encoding\n","label_encoder = LabelEncoder()\n","df['encoded_label'] = label_encoder.fit_transform(df['label'])\n","\n","# TF-IDF Representation\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(df['clean_text'])\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n","\n","# Save Outputs\n","df[['clean_text', 'label', 'encoded_label']].to_csv('cleaned_data.csv', index=False)\n","tfidf_df.to_csv('tfidf_representation.csv', index=False)\n","\n","print(\"Outputs saved successfully.\")\n"]}]}