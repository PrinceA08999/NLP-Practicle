{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNndebZlHt8MT70qKPFkqx6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-F5YjgdWAndn","executionInfo":{"status":"ok","timestamp":1714044557885,"user_tz":-330,"elapsed":2342,"user":{"displayName":"Prince A (Adhikesh Somwanshi)","userId":"09578747815701702608"}},"outputId":"e92d3d16-6d30-41d6-c5c0-002e38ff7ef4"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Whitespace Tokenizer: ['Tokenization', 'is', 'a', 'key', 'task', 'in', 'NLP.', 'It', 'breaks', 'text', 'into', 'tokens,', 'which', 'can', 'be', 'words,', 'phrases,', 'or', 'symbols.']\n","Punctuation-based Tokenizer: ['Tokenization', 'is', 'a', 'key', 'task', 'in', 'NLP', '.', 'It', 'breaks', 'text', 'into', 'tokens', ',', 'which', 'can', 'be', 'words', ',', 'phrases', ',', 'or', 'symbols', '.']\n","Treebank Tokenizer: ['Tokenization', 'is', 'a', 'key', 'task', 'in', 'NLP.', 'It', 'breaks', 'text', 'into', 'tokens', ',', 'which', 'can', 'be', 'words', ',', 'phrases', ',', 'or', 'symbols', '.']\n","Tweet Tokenizer: ['Tokenization', 'is', 'a', 'key', 'task', 'in', 'NLP', '.', 'It', 'breaks', 'text', 'into', 'tokens', ',', 'which', 'can', 'be', 'words', ',', 'phrases', ',', 'or', 'symbols', '.']\n","MWE Tokenizer: ['Tokenization', 'is', 'a', 'key_task', 'in', 'NLP.', 'It', 'breaks', 'text', 'into', 'tokens,', 'which', 'can', 'be', 'words,', 'phrases,', 'or', 'symbols.']\n","Porter Stemmer: ['token', 'is', 'a', 'key', 'task', 'in', 'nlp.', 'it', 'break', 'text', 'into', 'token', ',', 'which', 'can', 'be', 'word', ',', 'phrase', ',', 'or', 'symbol', '.']\n","Snowball Stemmer: ['token', 'is', 'a', 'key', 'task', 'in', 'nlp.', 'it', 'break', 'text', 'into', 'token', ',', 'which', 'can', 'be', 'word', ',', 'phrase', ',', 'or', 'symbol', '.']\n","Lemmatization: ['Tokenization', 'is', 'a', 'key', 'task', 'in', 'NLP.', 'It', 'break', 'text', 'into', 'token', ',', 'which', 'can', 'be', 'word', ',', 'phrase', ',', 'or', 'symbol', '.']\n"]}],"source":["import nltk\n","from nltk.tokenize import WhitespaceTokenizer, WordPunctTokenizer, TreebankWordTokenizer, TweetTokenizer\n","from nltk.tokenize import MWETokenizer\n","from nltk.stem import PorterStemmer, SnowballStemmer\n","from nltk.stem.wordnet import WordNetLemmatizer\n","nltk.download('wordnet')\n","\n","# Sample text\n","text = \"Tokenization is a key task in NLP. It breaks text into tokens, which can be words, phrases, or symbols.\"\n","\n","# Tokenization\n","# Whitespace Tokenizer\n","whitespace_tokenizer = WhitespaceTokenizer()\n","whitespace_tokens = whitespace_tokenizer.tokenize(text)\n","print(\"Whitespace Tokenizer:\", whitespace_tokens)\n","\n","# Punctuation-based Tokenizer\n","punct_tokenizer = WordPunctTokenizer()\n","punct_tokens = punct_tokenizer.tokenize(text)\n","print(\"Punctuation-based Tokenizer:\", punct_tokens)\n","\n","# Treebank Tokenizer\n","treebank_tokenizer = TreebankWordTokenizer()\n","treebank_tokens = treebank_tokenizer.tokenize(text)\n","print(\"Treebank Tokenizer:\", treebank_tokens)\n","\n","# Tweet Tokenizer\n","tweet_tokenizer = TweetTokenizer()\n","tweet_tokens = tweet_tokenizer.tokenize(text)\n","print(\"Tweet Tokenizer:\", tweet_tokens)\n","\n","# Multi-Word Expression Tokenizer\n","mwe_tokenizer = MWETokenizer()\n","mwe_tokenizer.add_mwe((\"key\", \"task\"))\n","mwe_tokens = mwe_tokenizer.tokenize(text.split())\n","print(\"MWE Tokenizer:\", mwe_tokens)\n","\n","# Stemming\n","porter_stemmer = PorterStemmer()\n","snowball_stemmer = SnowballStemmer(\"english\")\n","\n","porter_stems = [porter_stemmer.stem(token) for token in treebank_tokens]\n","print(\"Porter Stemmer:\", porter_stems)\n","\n","snowball_stems = [snowball_stemmer.stem(token) for token in treebank_tokens]\n","print(\"Snowball Stemmer:\", snowball_stems)\n","\n","# Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [lemmatizer.lemmatize(token) for token in treebank_tokens]\n","print(\"Lemmatization:\", lemmatized_tokens)\n"]}]}